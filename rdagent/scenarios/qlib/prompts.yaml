hypothesis_and_feedback: |-
  =========================================================
  {% for experiment, feedback in trace.hist %}
  # Trial {{ loop.index }}: 
  ## Hypothesis
  {{ experiment.hypothesis }}
  ## Specific task: 
  {% for task in experiment.sub_tasks %}
  {% if task is not none and task.get_task_brief_information is defined %}
  {{ task.get_task_brief_information() }}
  {% endif %}
  {% endfor %}
  ## Market Backtest Analysis and Feedback:
  {% if experiment.result is not none %}
  Backtest Result: {{ experiment.result.loc[["IC", "1day.excess_return_without_cost.annualized_return", "1day.excess_return_without_cost.max_drawdown"]] }}
  {% endif %}
  Observation (Key findings from the experiment): {{ feedback.observations }}
  Hypothesis Evaluation (Analysis of the original hypothesis): {{ feedback.hypothesis_evaluation }}
  Decision (Whether the hypothesis was successful): {{ feedback.decision }}
  =========================================================
  {% endfor %}

last_hypothesis_and_feedback: |-
  ## Hypothesis
  {{ experiment.hypothesis }}
  ## Specific task: 
  {% for task in experiment.sub_tasks %}
  {% if task is not none and task.get_task_brief_information is defined %}
  {{ task.get_task_brief_information() }}
  {% endif %}
  {% endfor %}
  ## Market Backtest Analysis and Feedback:
  {% if experiment.result is not none %}
  Backtest Result: {{ experiment.result.loc[["IC", "1day.excess_return_without_cost.annualized_return", "1day.excess_return_without_cost.max_drawdown"]] }}
  {% endif %}
  Observation (Key findings from the experiment): {{ feedback.observations }}
  Hypothesis Evaluation (Analysis of the original hypothesis): {{ feedback.hypothesis_evaluation }}
  Decision (Whether the hypothesis was successful): {{ feedback.decision }}
  New Hypothesis (Given in feedback stage, can be accepted or rejected in the next round): {{ feedback.new_hypothesis }}
  Reasoning (Justification for the new hypothesis): {{ feedback.reason }}

hypothesis_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
  "hypothesis": "An exact, testable, and innovative statement derived from previous experimental trace analysis. Avoid overly general ideas and ensure precision. The hypothesis should clearly specify the exact approach and expected improvement in performance in two or three sentences.",
  "reason": "Provide a clear, logical explanation for why this hypothesis was proposed, grounded in evidence (e.g., trace history, domain principles). Reason should be short with no more than two sentences.",
  "concise_reason": "A two-sentence summary. First sentence provides a clear justification for the change. Second sentence states a generalizable knowledge principle.",
  "concise_observation": "A single-sentence summary focusing on key observations from the scenario, data characteristics, or past experiences (both successes and failures).",
  "concise_justification": "A single-sentence summary that explains the hypothesis based on theoretical principles or initial assumptions.",
  "concise_knowledge": "A single-sentence summary of transferable knowledge based on theoretical principles. Use conditional statements (e.g., 'If..., then...; When..., then...') to state clear, unambiguous relationships."
  }

model_hypothesis_specification: |-
  Additional Specifications:
    1. Hypotheses should evolve incrementally. If there is no prior hypothesis, begin with a simple architecture. Subsequent hypotheses must build upon the previous one, incorporating feedback and observed performance.
    2. Focus strictly on the architecture of PyTorch models. Each hypothesis must address architectural decisions, such as layer configurations, activation functions, regularization methods, and overall model structure.
    3. Do not include aspects unrelated to architecture, such as input features or optimization strategies.
    4. Custom architecture design is encouraged. You may use standard libraries, BUT IT IS PREFERRED TO DEFINE COMPONENTS MANUALLY TO EXPLORE NOVEL STRUCTURES.

  Logic for Hypothesis Development:
    1. If the previous hypothesis outperforms the prior state of the art, inherit its structure and increase model depth or complexity.
    2. If the previous hypothesis performs poorly, you should judge based on the reasoning and feedback whether to revise the architecture or increasing model's size.

factor_hypothesis_specification: |-
  0. **IMPORTANT GUIDELINES**
    - Keep in mind that factors surpassing the SOTA are included in the SOTA factor library to avoid redundant re-implementation.
    - When writing a new hypothesis, be specific about the exact methods to be used, rather than using generalized concepts like machine learning models or ensembles.
    - Prioritize factors that are likely to yield high Information Coefficient based on empirical intuition or prior evidence.
    - Ensure each generation produces 1-5 factors. Balance simplicity and complexity to build a robust factor library.

  1. **Type of Factor and Financial Trends:**
    - Define the type of factor introduced.
    - Omit unnecessary or redundant details.
  
  2. **Simple and Effective Factors First:**
    - Start with factors that are simple and likely effective.
    - Concisely explain why these factors are expected to work.
    - Avoid complex or combined factors initially.
  3. **Gradual Complexity Increase:**
    - Introduce more complex factors (e.g. Machine Learning based factors) as more experimental results are gathered.
    - Discuss potential advantages and complexities.
    - Combine factors only after simpler ones are tested and validated.

  4. **New Directions and Optimizations:**
    - If a new direction is needed (repeated improvements fail to yield better results), explain why based on financial principles, economic theories, or market behaviors.
    - Suggest only one new direction at a time for clarity. The new direction can be entirely different from previous factors.
    - If a previous hypothesis did not surpass SOTA but seems optimizable, you may continue in the same direction.


factor_experiment_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
      "factor name 1": {
          "description": "description of factor 1, start with its type, e.g. [Momentum Factor]",
          "formulation": "latex formulation of factor 1",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      },
      "factor name 2": {
          "description": "description of factor 2, start with its type, e.g. [Machine Learning based Factor]",
          "formulation": "latex formulation of factor 2",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      }
      # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
  }

model_experiment_output_format: |-
  So far please only design one model to test the hypothesis! 
  The output should follow JSON format. The schema is as follows: 
  {
    "model_name 1 (The name of the model)": {
        "description": "A detailed description of the model",
        "formulation": "A LaTeX formula representing the model's formulation",
        "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "variable_name_2": "Description of variable 2",
            "variable_name_3": "Description of variable 3"
        },
        "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
        },
        "model_type": "Tabular or TimeSeries"  # Should be one of "Tabular" or "TimeSeries"
    },
    "model_name 2 (The name of the model)": {
        ...
    }
  }

factor_feedback_generation:
  system: |-
    You are a professional financial result analysis assistant in data-driven R&D. 
    The task is described in the following scenario:

    {{ scenario }}
    
    You will receive a hypothesis, multiple tasks with their factors, their results, and the SOTA result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous SOTA (State of the Art) results, and suggest improvements or new directions.
    
    Please understand the following operation logic and then make your feedback that is suitable for the scenario:
      1. Logic Explanation:
          - If the previous hypothesis factor surpasses the SOTA, include this factor in the SOTA factor library.
          - New experiments will generate new factors, which will be combined with the factors in the SOTA library.
          - These combined factors will be backtested and compared against the current SOTA to continuously iterate.
      2. Development Directions:
          - New Direction:
              - Propose a new factor direction for exploration and development.
          - Optimization of Existing Direction:
              - If the previous experiment's factor replaced the SOTA, suggest further improvements to that factor.
              - Clearly specify the differences in name and improvements compared to the previous factor.
          - Continued Research:
              - If the previous experiment's factor did not replace the SOTA, suggest ways to optimize and develop factors in this direction.
      3. Final Goal:
          - The ultimate goal is to continuously accumulate factors that surpass each iteration to maintain the best SOTA.
    
      When judging the results:
      1. **Recommendation for Replacement:**
        - If the new factor shows a significant improvement in the annualized return without transaction costs, recommend it to replace the current best result.
        - If the annualized return and any other single metric are better than SOTA, recommend the replacement.
        - Minor variations in other metrics are acceptable as long as the annualized return improves.

    Consider Changing Direction for Significant Gaps with SOTA:
      - If the new results significantly differ from the SOTA, consider exploring a new direction.
      - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.

    Please provide detailed and constructive feedback for future exploration.
    Respond in JSON format. Example JSON structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }
  user: |-
    Target hypothesis: 
    {{ hypothesis_text }}
    Tasks and Factors:
    {% for task in task_details %}
      - {{ task.factor_name }}: {{ task.factor_description }}
        - Factor Formulation: {{ task.factor_formulation }}
        - Variables: {{ task.variables }}
        - Factor Implementation: {{ task.factor_implementation }}
        {% if task.factor_implementation == "False" %}
        **Note: This factor was not implemented in the current experiment. Only the hypothesis for implemented factors can be verified.**
        {% endif %}
    {% endfor %}
    Combined Results: 
    {{ combined_result }}
    
    Analyze the combined result in the context of its ability to:
    1. Support or refute the hypothesis.
    2. Show improvement or deterioration compared to the SOTA experiment.

    Evaluation Metrics Explanations:
    Below are the financial meanings of each metric, which should be used to judge the results:

    - 1day.excess_return_without_cost.annualized_return: Annualized return without considering transaction costs. (the bigger the better)
    - IC: Measures the correlation between predicted returns (\hat{y}) and actual returns (y), using Pearson correlation. (the bigger the better)

    When judging the results:
    1. **Recommendation for Replacement:**
      - If new factor's IC is greater than SOTA's IC, recommend the replacement (IC is the most important metric for factor development).
      - If new factor's annualized return is greater than SOTA's annualized return, recommend the replacement.

    Consider Changing Direction for Significant Gaps with SOTA:
      - If the new results significantly differ from the SOTA, consider exploring a new direction.
      - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.

    Note: Only factors with 'Factor Implementation' as True are implemented and tested in this experiment. If 'Factor Implementation' is False, the hypothesis for that factor cannot be verified in this run.

model_feedback_generation:
  system: |-
    You are a professional quantitative analysis assistant in top-tier hedge fund.

    The task is described in the following scenario:
    {{ scenario }}

    You will receive a quantitative model hypothesis, its specific task description, and it market backtest result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous SOTA (State of the Art) results, and suggest improvements or new directions.


    Please provide detailed and constructive feedback. Note that as hypothesis evolve, a general trend should be that the model grows larger. 
    Example JSON Structure for Result Analysis:
    {
      "Observations": "Clearly summarize current and SOTA results with exact scores and notable patterns. Limit to no more than three concise, data-focused sentences.",
      "Feedback for Hypothesis": "Explicitly confirm or refute the hypothesis based on specific data points or performance trends. Limit to two sentences.",
      "New Hypothesis": "Propose a revised hypothesis, considering observed patterns and limitations in the current one. Limit to no more than two sentences.",
      "Reasoning": "Explain the rationale for the new hypothesis using specific trends or performance shifts. Be concise but technically complete. Limit to two sentences.",
      "Decision": <true or false>,
    }

    
  user: |-

    {% if sota_hypothesis %} 
    # SOTA Round Information:
    Hypothesis: {{ sota_hypothesis.hypothesis }}
    Specific Task: {{ sota_task }}
    Code Implementation: {{ sota_code }}
    Result: {{ sota_result }}
    {% else %}
    # This is the first round. No previous information available. As long as the performance is not too negative (eg.ICIR is greater than 0), treat it as successful. Do not set the threshold too high.  
    {% endif %} 
    
    # Current Round Information:
    Hypothesis: {{ hypothesis.hypothesis }}
    Why propose this hypothesis: {{ hypothesis.reason }}
    Specific Task: {{ exp.sub_tasks[0] }}
    Code Implementation: {{ exp.sub_workspace_list[0].file_dict.get("model.py") }}
    Result: {{ exp.result }}
    # TODO
    Execution Log: {{ exp.stdout }}

    # Evaluation Metrics Explanations:
    Below are the financial meanings of each metric, which should be used to judge the results:

    - 1day.excess_return_without_cost.annualized_return: Annualized return without considering transaction costs. (the bigger the better)
    - IC: Measures the correlation between predicted returns (\hat{y}) and actual returns (y), using Pearson correlation. (the bigger the better)

    # When judging the results:
    # TODO
    1. First, focus on the model's training process to determine whether the current parameters are reasonable and if there is a need for optimization.
    2. **Recommendation for Replacement:**
      - If the new model's performance shows a significant improvement in the annualized return without transaction costs, recommend it to replace the current SOTA result.
      - If the annualized return and any other single metric are better than SOTA, recommend the replacement.
      - Minor variations in other metrics are acceptable as long as the annualized return improves.
    3.  Consider Changing Direction When Results Are Significantly Worse Than SOTA:
      - If the new results significantly worse than the SOTA, consider exploring a new direction.
