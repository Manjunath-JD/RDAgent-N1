hypothesis_gen: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    The user is working on generating new hypotheses for the {{ targets }} in a data-driven research and development process. 
    The {{ targets }} are used in the following scenario:
    {{ scenario }}
    
    The user has already proposed several hypotheses and conducted evaluations. This information will be provided to you. Your task is to:
    1. Review the existing hypotheses and their evaluation results: Determine if any existing hypotheses are valid and worth pursuing further.
    2. Decide on the next step: Based on the results and reasoning, decide whether:
      - To propose a new direction, diverging from the current focus.
      - To refine and deepen the exploration of the current hypothesis or direction.
    3. If refining an existing hypothesis: Provide clear adjustments or additional details to enhance its focus.
    4. If proposing a new hypothesis: Ensure it is distinct and addresses any gaps or shortcomings in the current approach.

    The current component to focus on is: {{ component }}.
    {% if hypothesis_specification %}
    To assist in hypothesis formulation, the user has provided additional information: {{ hypothesis_specification }}.
    Important: If the hypothesis_specification outlines specific next steps, ensure that you follow those instructions carefully.
    {% endif %}
    Please generate the output using the following format and specifications:
    {{ hypothesis_output_format }}

  user: |-
    {% if exp_and_feedback_desc|length == 0 %}
    This is the first round of hypothesis generation. The user has not yet proposed any hypotheses for this scenario.
    {% else %}
    This is not the first round. The user has already proposed several hypotheses and conducted evaluations.
    
    The previous hypotheses and their corresponding feedback are as follows (focus on the most recent hypothesis, its derived insights, and reasoning):
    {{ exp_and_feedback_desc }}
    {% endif %}
    
    In addition, generate relevant reasoning and distilled knowledge keys.
    For these keys, especially the knowledge section, provide detailed context specific to the scenario to enhance domain understanding, rather than offering general knowledge.

hypothesis_model: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    The user is working on generating new hypotheses for the {{ targets }} in a data-driven research and development process. 
    The {{ targets }} are used in the following scenario:
    {{ scenario }}
    {% if model_enough %}
    There are sufficient models available ({{ model_info | length }} models). Your task is to choose one of the existing models for further tuning or optimization. Based on the model's information:
    {{ model_info }}
    Ensure the hypothesis is specific, actionable, and well-justified.
    {% else %}
    The number of available models is insufficient ({{ model_info | length }} models). Your task is to first decide whether to:
    - Tune an existing model: Select one of the current models for further tuning and improvement.
    - Add a new model: Introduce a new model to expand the hypothesis space.
    Based on the current model information:
    {{ model_info }}
    Make a decision and proceed accordingly:
    - If you decide to tune an existing model, select the most promising one and generate a new hypothesis.
    - If you decide to add a new model, specify the type of model you would add and generate a new hypothesis related to the new model.
    {% endif %}
    {% if hypothesis_specification %}
    To assist in hypothesis formulation, the user has provided additional information: {{ hypothesis_specification }}.
    Important: If the hypothesis_specification outlines specific next steps, ensure that you follow those instructions carefully.
    {% endif %}
    Please generate the output using the following format and specifications:
    {{ hypothesis_output_format }}

hypothesis_and_feedback: |-
  {% for experiment, feedback in hist %}
  Hypothesis {{ loop.index }}
  The experiment is design driven by hypothesis : {{ experiment.hypothesis }}
  Observation on the result with the hypothesis: {{ feedback.observations }}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

task_gen:
  system: |-
    {% if hypothesis is not none %}
    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step. 
    {% else %}
    The user is trying to generate a very simple new {{ targets }} based on the information provided. 
    {% endif %}
    The {{ targets }} are used in certain scenario, the scenario is as follows:
    {{ scenario }}

    {% if task_specification is not none %}
    The user has wrote some specification for the {{ targets }}. The specification is as follows:
    {{ task_specification }}
    Your task should adhere to the specification above.
    {% endif %}

    {% if hypothesis is none %}
    Since we are at the very beginning stage, we plan to start from a very simple task. To each component, please only generate the task to implement the most simple and basic function of the component. For example, the feature engineering should only implement the function which output the raw data without any transformation. The model component only uses the most basic and easy to implement model without any tuning. The ensemble component only uses the simplest ensemble method. The main focus at this stage is to build the first runnable version of the solution.
    {% else %}
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.
    {% endif %}

    Please generate the output following the format below:
    {{ task_output_format }}
    
  user: |-
    {% if workspace_code %}
    Here is a list of all the filenames and their corresponding content in the workspace:
    {{workspace_code}}
    {% endif %}

    {% if former_task_desc is not none %}
    The user has made several task on this scenario but didn't get the expected result due to wrong implementation or just bad luck. The former task is as follows:
    {{ former_task_desc }}
    Please avoid generating similar task to the former task to avoid the same mistake and boost efficiency.
    
    {% if targets == "Model" %}
    Based on the feedback from previous experiment failures, if the failure was due to exceeding the time limit or memory constraints, start with the smallest model size or choose alternative algorithms or methods with significantly lower time or space complexity instead of using a neural network. You can then iteratively refine and optimize the model in later stages.
    {% endif %}
    
    {% endif %}

    {% if hypothesis is not none %}
    The user has made several hypothesis on this scenario and did several evaluation on them.
    The target hypothesis you are targeting to generate {{ targets }} for is as follows:
    {{ hypothesis }}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ exp_and_feedback_desc }}
    Please generate the new {{ targets }} based on the information above.
    {% else %}
    Please generate the new {{ targets }} task.
    {% endif %}

task_gen_model: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    {% if hypothesis is not none %}
    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step. 
    {% else %}
    The user is trying to generate new {{ targets }} based on the information provided. 
    {% endif %}
    The {{ targets }} are used in certain scenario, the scenario is as follows:
    {{ scenario }}

    {% if hypothesis is not none %}
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.
    {% endif %}
    Please generate the output following the format below:
    {{ task_output_format }}
    
  user: |-
    {% if hypothesis is not none %}
    The user has made several hypothesis on this scenario and did several evaluation on them.
    The target hypothesis you are targeting to generate {{ targets }} for is as follows:
    {{ hypothesis }}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ exp_and_feedback_desc }}
    Please generate the new {{ targets }} based on the information above.
    {% else %}
    Please generate the new {{ targets }} task.
    {% endif %}

direct_exp_gen:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science, and also a grandmaster in Kaggle competitions.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.
    
    The user is working on creating a solution for a Kaggle competition. Your task is to first suggest a hypothesis and then design a task to enhance the current best solution based on that hypothesis.

    The component to focus on for the next hypothesis is already determined as: {{ component }}.
    It will be used in the following scenario:
    {{ scenario }}

    To assist you generate better ideas, the code below is a solution which get good grades(which scored 0.21 at test dataset) for reference:
    ```python
    import os
    import pandas as pd
    import numpy as np
    import torch
    from torch.utils.data import Dataset, DataLoader
    from transformers import (
        BertTokenizer,
        BertForSequenceClassification,
        RobertaTokenizer,
        RobertaForSequenceClassification,
        XLNetTokenizer,
        XLNetForSequenceClassification,
        ElectraTokenizer,
        ElectraForSequenceClassification,
        DebertaV2Tokenizer,
        DebertaV2ForSequenceClassification,
        AdamW,
        get_cosine_schedule_with_warmup,
    )
    import torch.nn.functional as F
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import log_loss
    import torch.nn as nn
    from tqdm import tqdm
    from sklearn.model_selection import train_test_split  # Importing train_test_split

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load data
    train_df = pd.read_csv("./input/train.csv")
    test_df = pd.read_csv("./input/test.csv")

    # Encode labels
    le = LabelEncoder()
    train_df["author_encoded"] = le.fit_transform(train_df["author"])

    # Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        train_df["text"],
        train_df["author_encoded"],
        test_size=0.1,
        random_state=42,
        stratify=train_df["author_encoded"],
    )

    # Parameters
    batch_size = 8  # Default batch size
    epochs = 3
    max_length = 160
    num_warmup_steps = 100


    # Define Focal Loss with gamma=1.0
    class FocalLoss(nn.Module):
        def __init__(self, gamma=1.0, reduction="mean"):
            super(FocalLoss, self).__init__()
            self.gamma = gamma
            self.reduction = reduction

        def forward(self, logits, target):
            ce_loss = nn.CrossEntropyLoss(reduction="none")(logits, target)
            pt = torch.exp(-ce_loss)
            focal_loss = ((1 - pt) ** self.gamma) * ce_loss
            if self.reduction == "mean":
                return torch.mean(focal_loss)
            else:
                return torch.sum(focal_loss)


    criterion = FocalLoss(gamma=1.0)

    # Ensure submission directory exists
    os.makedirs("./submission", exist_ok=True)

    # Prepare submission dataframe
    submission = pd.DataFrame()
    submission["id"] = test_df["id"]

    # Lists to collect individual model predictions and validation losses
    individual_val_preds = []
    individual_test_preds = []
    val_log_losses = []

    # Set val_targets once before the loop
    val_targets = val_labels.values

    # List of models to ensemble
    model_infos = [
        {
            "name": "bert-base-uncased",
            "tokenizer_class": BertTokenizer,
            "model_class": BertForSequenceClassification,
            "config": {
                "hidden_dropout_prob": 0.0,
                "attention_probs_dropout_prob": 0.0,
            },
            "batch_size": batch_size,
        },
        {
            "name": "roberta-base",
            "tokenizer_class": RobertaTokenizer,
            "model_class": RobertaForSequenceClassification,
            "config": {
                "hidden_dropout_prob": 0.0,
                "attention_probs_dropout_prob": 0.0,
            },
            "batch_size": batch_size,
        },
        {
            "name": "xlnet-base-cased",
            "tokenizer_class": XLNetTokenizer,
            "model_class": XLNetForSequenceClassification,
            "config": {
                "summary_last_dropout": 0.0,
            },
            "batch_size": batch_size,
        },
        {
            "name": "google/electra-base-discriminator",
            "tokenizer_class": ElectraTokenizer,
            "model_class": ElectraForSequenceClassification,
            "config": {
                "hidden_dropout_prob": 0.0,
                "attention_probs_dropout_prob": 0.0,
            },
            "batch_size": batch_size,
        },
        {
            "name": "microsoft/deberta-v3-base",
            "tokenizer_class": DebertaV2Tokenizer,
            "model_class": DebertaV2ForSequenceClassification,
            "config": {
                "hidden_dropout_prob": 0.0,
                "attention_probs_dropout_prob": 0.0,
            },
            "batch_size": batch_size,
        },
        {
            "name": "microsoft/deberta-v3-large",
            "tokenizer_class": DebertaV2Tokenizer,
            "model_class": DebertaV2ForSequenceClassification,
            "config": {
                "hidden_dropout_prob": 0.0,
                "attention_probs_dropout_prob": 0.0,
            },
            "batch_size": 4,  # Reduce batch size for large model
        },
    ]

    for idx, model_info in enumerate(model_infos):
        print(f"Training model {idx+1}/{len(model_infos)}: {model_info['name']}")
        # Initialize tokenizer
        tokenizer = model_info["tokenizer_class"].from_pretrained(model_info["name"])

        # Tokenize texts
        def encode_texts(texts):
            return tokenizer.batch_encode_plus(
                texts.tolist(),
                max_length=max_length,
                padding="max_length",
                truncation=True,
                return_attention_mask=True,
                return_tensors="pt",
            )

        train_encodings = encode_texts(train_texts)
        val_encodings = encode_texts(val_texts)
        test_encodings = encode_texts(test_df["text"])

        # Create custom dataset
        class TextDataset(Dataset):
            def __init__(self, encodings, labels=None):
                self.encodings = encodings
                if labels is not None:
                    self.labels = labels.tolist()
                else:
                    self.labels = None

            def __getitem__(self, idx):
                item = {key: val[idx] for key, val in self.encodings.items()}
                if self.labels is not None:
                    item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
                return item

            def __len__(self):
                return len(self.encodings.input_ids)

        train_dataset = TextDataset(train_encodings, train_labels)
        val_dataset = TextDataset(val_encodings, val_labels)
        test_dataset = TextDataset(test_encodings)

        # Data loaders with model-specific batch size
        current_batch_size = model_info.get("batch_size", batch_size)
        train_loader = DataLoader(
            train_dataset, batch_size=current_batch_size, shuffle=True
        )
        val_loader = DataLoader(val_dataset, batch_size=current_batch_size)
        test_loader = DataLoader(test_dataset, batch_size=current_batch_size)

        # Initialize model
        if model_info["name"] == "xlnet-base-cased":
            config = (
                model_info["model_class"]
                .from_pretrained(
                    model_info["name"],
                    num_labels=3,
                    summary_last_dropout=0.0,
                )
                .config
            )
        elif model_info["name"] in [
            "google/electra-base-discriminator",
            "microsoft/deberta-v3-base",
            "microsoft/deberta-v3-large",
        ]:
            config = (
                model_info["model_class"]
                .from_pretrained(
                    model_info["name"],
                    num_labels=3,
                    hidden_dropout_prob=0.0,
                    attention_probs_dropout_prob=0.0,
                )
                .config
            )
        else:
            config = (
                model_info["model_class"]
                .from_pretrained(
                    model_info["name"],
                    num_labels=3,
                    **model_info["config"],
                )
                .config
            )

        model = model_info["model_class"].from_pretrained(
            model_info["name"],
            config=config,
        )
        model.to(device)

        # Optimizer and scheduler
        total_steps = len(train_loader) * epochs
        optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
        scheduler = get_cosine_schedule_with_warmup(
            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps
        )

        # Training loop
        for epoch in range(epochs):
            model.train()
            total_loss = 0
            for batch in tqdm(train_loader, desc=f"Epoch {epoch +1}"):
                optimizer.zero_grad()
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                if (
                    "token_type_ids" in batch
                    and "token_type_ids" in model.forward.__code__.co_varnames
                ):
                    token_type_ids = batch["token_type_ids"].to(device)
                else:
                    token_type_ids = None
                labels = batch["labels"].to(device)
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    return_dict=True,
                )
                logits = outputs.logits
                loss = criterion(logits, labels)
                total_loss += loss.item()
                loss.backward()
                optimizer.step()
                scheduler.step()
            avg_train_loss = total_loss / len(train_loader)
            print(f"Epoch {epoch +1}, Training loss: {avg_train_loss}")

        # Evaluation
        model.eval()
        val_preds = []
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Evaluating"):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                if (
                    "token_type_ids" in batch
                    and "token_type_ids" in model.forward.__code__.co_varnames
                ):
                    token_type_ids = batch["token_type_ids"].to(device)
                else:
                    token_type_ids = None
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    return_dict=True,
                )
                logits = outputs.logits
                probs = F.softmax(logits, dim=-1)
                val_preds.extend(probs.cpu().numpy())
        val_preds = np.array(val_preds)
        individual_val_preds.append(val_preds)

        # Calculate validation log loss for this model
        val_log_loss_i = log_loss(val_targets, val_preds)
        val_log_losses.append(val_log_loss_i)
        print(f"Model {idx+1} Validation Log Loss: {val_log_loss_i}")

        # Predictions on test set
        model.eval()
        test_preds = []
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Predicting"):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                if (
                    "token_type_ids" in batch
                    and "token_type_ids" in model.forward.__code__.co_varnames
                ):
                    token_type_ids = batch["token_type_ids"].to(device)
                else:
                    token_type_ids = None
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    return_dict=True,
                )
                logits = outputs.logits
                probs = F.softmax(logits, dim=-1)
                test_preds.extend(probs.cpu().numpy())
        test_preds = np.array(test_preds)
        individual_test_preds.append(test_preds)

        # Clean up
        del model
        torch.cuda.empty_cache()

    # Compute weights inversely proportional to validation log losses
    inv_val_log_losses = [1 / loss for loss in val_log_losses]
    weights = inv_val_log_losses / np.sum(inv_val_log_losses)
    print(f"Weights: {weights}")

    # Weighted ensemble of validation predictions
    ensemble_val_preds = np.zeros_like(individual_val_preds[0])
    for weight, val_pred in zip(weights, individual_val_preds):
        ensemble_val_preds += weight * val_pred

    # Calculate overall validation log loss
    overall_val_log_loss = log_loss(val_targets, ensemble_val_preds)
    print(f"Overall Validation Log Loss: {overall_val_log_loss}")

    # Weighted ensemble of test predictions
    ensemble_test_preds = np.zeros_like(individual_test_preds[0])
    for weight, test_pred in zip(weights, individual_test_preds):
        ensemble_test_preds += weight * test_pred

    # Prepare submission
    submission_probs = ensemble_test_preds / np.sum(
        ensemble_test_preds, axis=1, keepdims=True
    )
    submission[le.inverse_transform([0, 1, 2])] = submission_probs
    submission = submission[["id", "EAP", "HPL", "MWS"]]

    # Save submission file
    submission.to_csv("./submission/submission.csv", index=False)

    # Print the validation metric at the end
    print(f"Overall Validation Log Loss: {overall_val_log_loss}")
    ```
    You can imitate this solution and then try to improve your performance.

    # Step1: Hypothesis Proposal
    The user has already proposed several hypotheses and conducted evaluations on them. This information will be provided to you later.

    ## Hypothesis Specification
    To assist you in formulating new hypotheses, the user has provided some additional information: 
    {{ hypothesis_specification }}

    **If the chosen component is model**:
    - When the target component is model architecture selection, the primary hypothesis must explicitly prioritize using large-scale pre-trained models as the base architecture. 
    - Always start from state-of-the-art pretrained checkpoints and focus on efficient fine-tuning strategies rather than training from scratch, even if this requires adjusting other components like batch size or learning schedules to accommodate model size.

    ## Guidelines
    Important: If the Hypothesis Specification outlines the next steps you need to follow, ensure you adhere to those instructions.

    [Partial Response Format 1] Your generated output should contain key-value pairs adhering to the following format and specifications:
    {{ hypothesis_output_format }}
    Also generate the relevant keys for the reasoning and the distilled knowledge that follows. For those keys, in particular for knowledge, explain in the context of the specific scenario to build up domain knowledge in the specific field rather than general knowledge.

    # Step2: Task Design

    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step.

    ## Task Specification
    The scope of the {{ targets }} can be described by a interface specification as follows:
    ```markdown
    {{ task_specification }}
    ```

    ## Guidelines
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.

    [Partial Response Format 2] Your generated output should contain key-value pairs adhering to the following format and specifications:
    {{ task_output_format }}

    {% if workflow_check %}
    # Step3: Workflow update
    Since components have dependencies, the workflow should be updated to reflect the changes made to the target component. Please also decide whether the workflow needs to be updated and provide a brief description of the change task.
    [Partial Response Format 3] Your generated workflow description should be a simple text and the following agent will do the implementation. If you think the workflow should not be updated, just respond with "No update needed".
    {% endif %}

    Your response should contain two parts: the hypothesis proposal and the task design. Please follow the format and specifications provided below:
    {
      "hypothesis_proposal": [Partial Response Format 1],
      "task_design": [Partial Response Format 2],
      {% if workflow_check %}"workflow_update": [Partial Response Format 3], {% endif %}
    }

  user: |-
    # All former successful experiments and their feedbacks
    Below are all the experiments that surpassed the previous SOTA solutions along with their feedback. The current SOTA solution is the latest among these successful trials:
    {{ sota_exp_and_feedback_list_desc }}

    {% if failed_exp_and_feedback_list_desc %}
    # Several latest failed experiments and their feedbacks
    The user has conducted several recent experiments on this scenario, but they either encountered execution errors or failed to surpass the SOTA performance. The details of these failed experiments and their results are as follows:
    {{ failed_exp_and_feedback_list_desc }}
    
    {% if targets == "Model" %}
    Based on the feedback from previous experiment failures, if the failure was due to exceeding the time limit or memory constraints, start with the smallest model size or choose alternative algorithms or methods with significantly lower time or space complexity instead of using a neural network. You can then iteratively refine and optimize the model in later stages.

    Here is the SOTA solution:
    {{ sota_exp_desc }}
    Pay attention to the **Results** section. If there are sufficient models available and there is a model with a significantly worse score, consider removing that model. In this case, `model_name` in task_design should be the model you are going to remove (the name must be the same as the name in the model column in the **Results** section), and `description` should start with "Model removal".
    
    Otherwise, if the number of available models is insufficient. Your task is to first decide whether to:
      a. Tune an existing model: Select one of the current models for further tuning and improvement.
      b. Add a new model: Introduce a new model to expand the hypothesis space.

    The information of the model is described by the code of workspace.

    Then, based on your decision, proceed with the corresponding actions accordingly:
      a. If you decide to tune an existing model, select the existing model file and generate a new hypothesis.
      b. If you decide to add a new model, specify the type of model you would add and generate a new hypothesis related to the new model.

    When building the model, if the runtime permits, consider incorporating hyperparameter search methods to improve performance.
    {% endif %}
    
    {% endif %}
    
    {% if last_exp_diff %}
    # Here are the differences between the latest version of implementation and the current best version of implementation
    It is presented in diff format, highlighting changes from the best version to the latest version.
    {{ last_exp_diff }}
    {% endif %}

component_gen:
  system: |-
    You are a Kaggle Grander Master. You are going to provide a solution for a kaggle competition.

    # Here is the description of the competition scenario:
    {{ scenario }}

    # Here is the current best version of implementation:
    {{ sota_exp_desc }}
    [Notice] Pay attention to the **Results** section. If there is a model with a significantly worse score, consider removing that model.

    {% if last_exp_diff %}
    # Here are the differences between the latest version of implementation and the current best version of implementation
    It is presented in diff format, highlighting changes from the best version to the latest version.
    {{ last_exp_diff }}
    {% endif %}

    You will be provided the feedback for the latest implementation.

    Please select the component you are going to improve the sota implementation.
    # Here is the brief description of the components you can select:
    {{ component_desc }}

    Please generate the output in JSON format following the format below:
    {{ component_output_format }}

  user: |-
    Here's the former SOTA experiments and their feedbacks:
    {{ sota_exp_and_feedback_list_desc }}

    Also, here's the former failed experiments and their feedbacks:
    {{ failed_exp_and_feedback_list_desc }}

    All former trials and their feedbacks are provided in pandas DataFrame format. The user has already made several hypothesis on this scenario and did several evaluation on them:
    {{ component_and_feedback_df }}
    
    Please choose the most proper component to focus on based on the information above. Please balance the exploration and exploitation.
    Avoid selecting the same component more than 5 times in a row to ensure that the chosen component is not overly repetitive.


exp_and_feedback: |-
  {% for experiment, feedback in trace.hist[-10:] %}
  ## Experiment {{ loop.index }}
  Experiment are focusing on task: {{ experiment.pending_tasks_list[0][0] }}
  {% if experiment.hypothesis %}
  The experiment is design driven by hypothesis : {{ experiment.hypothesis }}
  Observation on the result with the hypothesis: {{ feedback.observations }}
  {% endif %}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

hypothesis_specification: |-
  1. The hypothesis should be precise, testable, and directly actionable. Avoid general or vague statements. For example, "tuning a model" is too broad, whereas "increasing the learning rate to 0.1 in the LightGBM model will improve performance" is specific and actionable.
  2. Each hypothesis should focus on a single direction per experiment. Avoid proposing multiple possibilities within the same hypothesis, such as "this may work in case A or case B." Research and development can be approached at different levels (shallow or deep), but each experimental loop should validate only one specific idea.
  3. The hypothesis should based on current SOTA solution. The user will conduct experiments based on the SOTA solution to test whether the hypothesis improves performance in this specific competition.

output_format:
  component: |-
    {
      "reason": "The reason why you choose this component. Based on the current status and former trials, why this component is the most promising one to focus on.",
      "component": "The component you suggest to focus on. It must be one of ['DataLoadSpec', 'FeatureEng', 'Model', 'Ensemble', 'Workflow']."
    }
  hypothesis: |-
    The output should follow JSON format. The schema is as follows:
    {
      "component": "If "hypothesis_specification" provides the component you need to take, please follow "hypothesis_specification" to choose the component. Otherwise, based on previous experimental results, suggest the component you believe is most appropriate at the moment. It should be one of ["DataLoadSpec", "FeatureEng", "Model", "Ensemble", "Workflow"]",
      "hypothesis": "The new hypothesis generated based on the information provided.",
      "reason": "The reason why you generate this hypothesis. It should be comprehensive and logical. It should cover the other keys below and extend them.",
      "concise_reason": "Two-line summary. First line focuses on a concise justification for the change. Second line generalizes a knowledge statement.",
      "concise_observation": "One line summary. It focuses on the observation of the given scenario, data characteristics, or previous experiences (failures & success).",
      "concise_justification": "One line summary. Justify the hypothesis based on theoretical principles or initial assumptions.",
      "concise_knowledge": "One line summary. Transferable knowledge based on theoretical principles. Use conditional grammar. eg. "If...., ..; When..., .; and etc" Make sure that you state things clearly without ambiguity. Eg. avoid saying "previous hypothesis", because one wouldn't know what that is."
    }
  data_loader: |-
    Design a specific and detailed data loader task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the overall data loader for the data science workflow",
        # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
    }
  feature: |-
    Design a specific and detailed feature engineering task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of feature engineering task",
        # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
    }
  model: |-
    Design a specific and detailed model task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows: 
    {
        "model_name": "model name, must start with 'model_' and only contain letters, numbers, and underscores",
        "description": "A precise and comprehensive description of the model. Start with [Model building/tuning] or [Model removal].",
    }
  ensemble: |-
    Design a specific and detailed ensemble task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the ensemble",
    }
  workflow: |-
    Design a specific and detailed workflow task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the workflow",
    }
