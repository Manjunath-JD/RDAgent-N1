{
    "PMLP": {
        "description": "`PMLP` is identical to a standard MLP during training, but then adopts a GNN architecture (add message passing) during testing.",
        "formulation": "\\hat{y}_u = \\psi(\\text{MP}(\\{h^{(l-1)}_v\\}_{v \\in N_u \\cup \\{u\\}}))",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "\\psi": "A function representing the feed-forward process, consisting of a linear feature transformation followed by a non-linear activation",
            "\\text{MP}": "Message Passing operation that aggregates neighbored information",
            "h^{(l-1)}_v": "The feature representation of node v at layer (l-1)",
            "N_u": "The set of neighbored nodes centered at node u"
        },
        "key": "pmlp",
        "model_type": "Graph"
    },
    "LINKX": {
        "description": "A scalable model for node classification that separately embeds adjacency and node features, combines them with MLPs, and applies simple transformations.",
        "formulation": "Y = MLP_f(\\sigma(W[h_A; h_X] + h_A + h_X))",
        "variables": {
            "Y": "The output predictions",
            "\\sigma": "Non-linear activation function",
            "W": "Learned weight matrix",
            "h_A": "Embedding of the adjacency matrix",
            "h_X": "Embedding of the node features",
            "MLP_f": "Final multilayer perceptron for prediction"
        },
        "key": "linkx",
        "model_type": "Graph"
    },
    "A-DGN": {
        "description": "A framework for stable and non-dissipative DGN design, conceived through the lens of ordinary differential equations (ODEs). It ensures long-range information preservation between nodes and prevents gradient vanishing or explosion during training.",
        "formulation": "\\frac{\\partial x_u(t)}{\\partial t} = \\sigma(W^T x_u(t) + \\Phi(X(t), N_u) + b)",
        "variables": {
            "x_u(t)": "The state of node u at time t",
            "\\frac{\\partial x_u(t)}{\\partial t}": "The rate of change of the state of node u at time t",
            "\\sigma": "A monotonically non-decreasing activation function",
            "W": "A weight matrix",
            "b": "A bias vector",
            "\\Phi(X(t), N_u)": "The aggregation function for the states of the nodes in the neighborhood of u",
            "X(t)": "The node feature matrix of the whole graph at time t",
            "N_u": "The set of neighboring nodes of u"
        },
        "key": "A-DGN",
        "model_type": "Graph"
    },
    "AlexNet": {
        "description": "A deep convolutional neural network (CNN) that achieved breakthrough performance in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It introduced several key innovations, including the use of ReLU activation, dropout, and data augmentation, which significantly improved image classification accuracy.",
        "formulation": [
            "Input → Conv1 → ReLU → LRN → MaxPool1 → Conv2 → ReLU → LRN → MaxPool2 → Conv3 → ReLU → Conv4 → ReLU → Conv5 → ReLU → MaxPool3 → FC6 → ReLU → Dropout → FC7 → ReLU → Dropout → FC8 → Softmax → Output"
        ],
        "variables": {
            "Input": "The input image (e.g., 227x227x3 for RGB images)",
            "Conv1": "First convolutional layer with 96 filters of size 11x11, stride 4",
            "ReLU": "Rectified Linear Unit activation function: max(0, x)",
            "LRN": "Local Response Normalization, applied after ReLU",
            "MaxPool1": "Max pooling layer with 3x3 filters, stride 2",
            "Conv2": "Second convolutional layer with 256 filters of size 5x5, stride 1",
            "MaxPool2": "Max pooling layer with 3x3 filters, stride 2",
            "Conv3": "Third convolutional layer with 384 filters of size 3x3, stride 1",
            "Conv4": "Fourth convolutional layer with 384 filters of size 3x3, stride 1",
            "Conv5": "Fifth convolutional layer with 256 filters of size 3x3, stride 1",
            "MaxPool3": "Max pooling layer with 3x3 filters, stride 2",
            "FC6": "First fully connected layer with 4096 units",
            "Dropout": "Dropout regularization with a dropout rate of 0.5",
            "FC7": "Second fully connected layer with 4096 units",
            "FC8": "Third fully connected layer with 1000 units (one for each ImageNet class)",
            "Softmax": "Softmax activation function for multi-class classification",
            "Output": "The predicted class probabilities"
        },
        "key": "AlexNet",
        "model_type": "Convolutional"
    },
    "ResNet": {
        "description": "Residual Networks (ResNets) are a type of deep neural network architecture that introduced skip connections to address the vanishing gradient problem in very deep networks. By enabling the direct flow of information across layers, ResNets allow for the training of extremely deep networks with improved performance.",
        "formulation": [
            "Input → Conv1 → BN → ReLU → MaxPool → ResBlock1 → ResBlock2 → ResBlock3 → AvgPool → FC → Softmax → Output"
        ],
        "variables": {
            "Input": "The input image (e.g., 224x224x3 for RGB images)",
            "Conv1": "Initial convolutional layer with 64 filters of size 7x7, stride 2",
            "BN": "Batch normalization applied after convolution",
            "ReLU": "Rectified Linear Unit activation function: max(0, x)",
            "MaxPool": "Max pooling layer with 3x3 filters, stride 2",
            "ResBlock1": "A stack of residual blocks with 64 filters",
            "ResBlock2": "A stack of residual blocks with 128 filters",
            "ResBlock3": "A stack of residual blocks with 256 filters",
            "AvgPool": "Average pooling layer",
            "FC": "Fully connected layer",
            "Softmax": "Softmax activation function for multi-class classification",
            "Output": "The predicted class probabilities"
        },
        "key": "ResNet",
        "model_type": "Convolutional"
    },
    "LeNet": {
        "description": "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues for handwritten digit recognition. It consists of multiple layers of convolution, pooling, and fully connected layers, demonstrating the effectiveness of deep learning for image classification tasks.",
        "formulation": [
            "Input → Conv1 → Tanh → AvgPool1 → Conv2 → Tanh → AvgPool2 → FC1 → Tanh → FC2 → Softmax → Output"
        ],
        "variables": {
            "Input": "The input image (e.g., 32x32x1 for grayscale images)",
            "Conv1": "First convolutional layer with 6 filters of size 5x5",
            "Tanh": "Hyperbolic tangent activation function",
            "AvgPool1": "Average pooling layer with 2x2 filters, stride 2",
            "Conv2": "Second convolutional layer with 16 filters of size 5x5",
            "AvgPool2": "Average pooling layer with 2x2 filters, stride 2",
            "FC1": "First fully connected layer with 120 units",
            "FC2": "Second fully connected layer with 84 units",
            "Softmax": "Softmax activation function for multi-class classification",
            "Output": "The predicted class probabilities"
        },
        "key": "GoogLeNet",
        "model_type": "Convolutional"
    },
    "RNN": {
        "description": "A type of neural network designed to capture sequential dependencies in data. It processes input sequences one element at a time, updating its hidden state at each step to retain information about past inputs.",
        "formulation": [
            "h_t = \\sigma(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})",
            "y_t = \\text{softmax}(W_{hy} h_t + b_{hy})"
        ],
        "variables": {
            "h_t": "The hidden state at time step t",
            "x_t": "The input at time step t",
            "W_{ih}, W_{hh}, W_{hy}": "Weight matrices for input-to-hidden, hidden-to-hidden, and hidden-to-output connections",
            "b_{ih}, b_{hh}, b_{hy}": "Bias vectors for input-to-hidden, hidden-to-hidden, and hidden-to-output connections",
            "\\sigma": "The activation function (e.g., sigmoid or tanh)",
            "\\text{softmax}": "The softmax function for multi-class classification",
            "y_t": "The output prediction at time step t"
        },
        "key": "RNN",
        "model_type": "Recurrent"
    },
    "LSTM": {
        "description": "A type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data. It uses gating mechanisms to control the flow of information, preventing gradient vanishing or explosion during training.",
        "formulation": [
            "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)",
            "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)",
            "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)",
            "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t",
            "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)",
            "h_t = o_t \\cdot \\tanh(C_t)"
        ],
        "variables": {
            "x_t": "The input at time step t",
            "h_t": "The hidden state at time step t",
            "C_t": "The cell state at time step t",
            "f_t": "The forget gate at time step t",
            "i_t": "The input gate at time step t",
            "\\tilde{C}_t": "The candidate cell state at time step t",
            "o_t": "The output gate at time step t",
            "W_f, W_i, W_C, W_o": "Weight matrices for the forget gate, input gate, cell state, and output gate, respectively",
            "b_f, b_i, b_C, b_o": "Bias vectors for the forget gate, input gate, cell state, and output gate, respectively",
            "\\sigma": "The sigmoid activation function",
            "\\tanh": "The hyperbolic tangent activation function"
        },
        "key": "LSTM",
        "model_type": "Recurrent"
    },
    "Seq2SeqEncoder": {
        "description": "A sequence-to-sequence (Seq2Seq) model encoder that processes input sequences and generates a fixed-size context vector to capture the input information. It is commonly used in tasks such as machine translation and text summarization.",
        "formulation": [
            "Input Sequence → Embedding Layer → Encoder RNN → Context Vector"
        ],
        "variables": {
            "Input Sequence": "The input sequence to be encoded (e.g., a sequence of words)",
            "Embedding Layer": "Converts input tokens into dense vectors",
            "Encoder RNN": "Recurrent neural network (e.g., LSTM or GRU) that processes the input sequence",
            "Context Vector": "A fixed-size representation of the input sequence"
        },
        "key": "Seq2SeqEncoderRNN",
        "model_type": "Recurrent"
    },
    "Seq2SeqDecoder": {
        "description": "A sequence-to-sequence (Seq2Seq) model decoder that generates an output sequence based on the context vector produced by the encoder. It typically uses an attention mechanism to focus on different parts of the input sequence during decoding.",
        "formulation": [
            "Context Vector → Decoder RNN → Output Sequence"
        ],
        "variables": {
            "Context Vector": "The fixed-size representation of the input sequence produced by the encoder",
            "Decoder RNN": "Recurrent neural network (e.g., LSTM or GRU) that generates the output sequence",
            "Output Sequence": "The predicted output sequence (e.g., a translated sentence)"
        },
        "key": "Seq2SeqDecoderRNN",
        "model_type": "Recurrent"
    },
    "TransformerEncoderModule": {
        "description": "A complete Transformer Encoder module consisting of multiple encoder layers, input embeddings, and positional encodings. Processes input sequences in parallel using stacked self-attention mechanisms and feed-forward networks to generate contextual representations.",
        "formulation": [
            "Input Tokens → Token Embedding → Positional Encoding → [Encoder Layer × N] → Output",
            "Each Encoder Layer:",
            "  - Multi-Head Self-Attention → Add & Norm",
            "  - Feed-Forward Network → Add & Norm"
        ],
        "variables": {
            "Input Tokens": "Discrete input tokens (e.g., words, subwords)",
            "Token Embedding": "Learned embedding layer converting tokens to vectors",
            "Positional Encoding": "Sinusoidal or learned position embeddings",
            "Encoder Layer": "Basic Transformer encoder layer with self-attention",
            "N": "Number of stacked encoder layers (typically 6-12)",
            "Output": "Contextualized sequence representation"
        },
        "key": "TransformerEncoderModule",
        "model_type": "Transformer"
    },
    "TransformerDecoderModule": {
        "description": "A complete Transformer Decoder module with multiple decoder layers, output embeddings, and positional encodings. Generates output sequences autoregressively using masked self-attention and encoder-decoder cross-attention mechanisms.",
        "formulation": [
            "Input Tokens → Token Embedding → Positional Encoding → [Decoder Layer × N] → Output Projection",
            "Each Decoder Layer:",
            "  - Masked Multi-Head Self-Attention → Add & Norm",
            "  - Multi-Head Cross-Attention (with encoder outputs) → Add & Norm",
            "  - Feed-Forward Network → Add & Norm"
        ],
        "variables": {
            "Input Tokens": "Target sequence tokens (shifted right for autoregression)",
            "Token Embedding": "Learned embedding layer for output tokens",
            "Positional Encoding": "Position information embedding",
            "Decoder Layer": "Complete decoder layer with three sub-layers",
            "N": "Number of stacked decoder layers",
            "Output Projection": "Linear layer mapping to vocabulary space",
            "Encoder Outputs": "Representations from encoder module (for cross-attention)"
        },
        "key": "TransformerDecoderModule",
        "model_type": "Transformer"
    },
    "SwimTransformerModule": {
        "description": "A complete Swim Transformer module for sequential data processing, featuring specialized position encoding and attention mechanisms optimized for temporal patterns in time series or audio data.",
        "formulation": [
            "Input Sequence → Projection Embedding → Enhanced Position Encoding → [Swim Layer × N] → Output",
            "Each Swim Layer:",
            "  - Temporal Position Encoding → Modified Self-Attention → Add & Norm",
            "  - Feed-Forward Network → Add & Norm"
        ],
        "variables": {
            "Input Sequence": "Raw sequential data (time series, audio frames, etc.)",
            "Projection Embedding": "Linear projection to embedding space",
            "Enhanced Position Encoding": "Specialized temporal position encoding",
            "Swim Layer": "Modified transformer layer with temporal attention",
            "N": "Number of stacked Swim layers",
            "Output": "Processed sequence representation"
        },
        "key": "SwimTransformerModule",
        "model_type": "Transformer"
    }
}