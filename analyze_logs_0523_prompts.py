import glob
import os

PROMPT_TEMPLATE = open("rdagent/scenarios/data_science/dev/prompts_v3.yaml").read()

competition_names = [
    "aerial-cactus-identification",
    "dogs-vs-cats-redux-kernels-edition",
    "hotel-id-2021-fgvc8",
    "mlsp-2013-birds",
    "rsna-miccai-brain-tumor-radiogenomic-classification",
    "text-normalization-challenge-english-language"
]


COMPETITION_NAME = "text-normalization-challenge-english-language"

STATISTICS = open(f"logs_0523/{COMPETITION_NAME}.csv").read()

USER_PROMPT = f"""
You are a data scientist specializing in analyzing log files.

RD-Agent (Research-Development Agent) is a tool that helps data scientists and machine learning engineers to automate the process of proposing new ideas, testing the new ideas, and evolving better solutions. It is currently designed to work in a Kaggle competition environment, where it loops to propose, test and give feedbacks. Since the Kaggle competition has only a private leaderboard, and there is only one chance to submit the prediction, RD-Agent is designed with a prompt to tell, in each loop, whether to replace the solution to be submitted with the current solution or not.

The prompt is generated by the following prompt template:

```yaml
{PROMPT_TEMPLATE}
```

Currently we have a competition (named {COMPETITION_NAME}). Within its dozens of loops, there are some iterations where the solution is good (possibly win medals), but the solution is later replaced by a worse solution and thus not submitted. In the following table, "Feedback = ✅" means that the solution is accepted by the prompt, while "Feedback = ❌" means that the solution is rejected by the prompt. The last solution with "Feedback = ✅" is the one that is finally submitted. But as you can see, the last solution does not necessarily have the best "Running Score (test)".

```csv
{STATISTICS}
```

You are seeing the concrete logs (prompts to decide whether to replace the solution or not) of every iteration in the loop. (some iterations might be missing due to not being able to finalize a valid solution that implements the idea) You will see the information we give to the LLM in the prompt (competition scenario, current sketch, code, etc.), and the instructions on how to check whether the solution is good or not, as well as the feedback from the LLM (whether to replace the solution or not).
Please analyze the logs and find out why the LLM rejected the good solutions and replaced them with worse solutions, and any chances that we can improve the original template to avoid this situation in the future.

In the output, please provide a detailed analysis, including:

1. Which are the iterations that are good but rejected by the LLM, and which are the iterations that are bad but accepted by the LLM.
2. What are the common patterns or reasons for the LLM's decisions in these iterations.
3. Any suggestions for improving the prompt template to avoid such situations in the future.
"""

LOGS_COMBINED = ""
for file in sorted(glob.glob(f"logs_0523/feedback_{COMPETITION_NAME}_loop*.log")):
    with open(file, "r") as f:
        header = "### " + os.path.basename(file).upper() + " ###"
        LOGS_COMBINED += "#" * len(header) + "\n" + header + "\n" + "#" * len(header) + "\n\n" + f.read() + "\n\n\n"


with open(f"logs_0523/prompt_user_{COMPETITION_NAME}.txt", "w") as f:
    f.write(USER_PROMPT)
with open(f"logs_0523/prompt_logs_{COMPETITION_NAME}.log", "w") as f:
    f.write(LOGS_COMBINED)
